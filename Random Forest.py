# -*- coding: utf-8 -*-
"""selected.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1u1GR5R1DJ1rbk3TA2zLmn53yrnNYJ_Ej
"""

!pip install pyspark
!pip install -U -q PyDrive2
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, countDistinct, desc

# Create a SparkSession
spark = SparkSession.builder \
    .appName("NetflixTitlesAnalysis") \
    .getOrCreate()

# Load the Netflix titles dataset
netflix_df = spark.read.csv("/content/netflix_titles.csv", header=True, inferSchema=True)

# 1. How many unique titles are there in the dataset?
unique_titles_count = netflix_df.select(countDistinct("title")).collect()[0][0]
print("Total unique titles:", unique_titles_count)

title_counts_by_year = netflix_df.groupBy("release_year").count().orderBy("release_year")
title_counts_by_year.show()

top_countries = netflix_df.groupBy("country").count().orderBy(desc("count")).limit(10)
top_countries.show()

# 4. What are the top 10 genres on Netflix?
top_genres = netflix_df.groupBy("listed_in").count().orderBy(desc("count")).limit(10)
top_genres.show()

average_movie_duration = netflix_df.filter(col("type") == "Movie").selectExpr("avg(duration)").collect()[0][0]
print("Average movie duration:", average_movie_duration)

average_seasons = netflix_df.filter(col("type") == "TV Show").selectExpr("avg(duration)").collect()[0][0]
print("Average number of seasons for TV shows:", average_seasons)

# 9. How has the number of titles added to Netflix evolved over the years?
titles_added_by_year = netflix_df.groupBy("release_year").count().orderBy("release_year")
titles_added_by_year.show()

top_directors = netflix_df.groupBy("director").count().orderBy(desc("count")).limit(10)
top_directors.show()

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml import Pipeline
from pyspark.ml.feature import Imputer

# Create a SparkSession
spark = SparkSession.builder \
    .appName("NetflixTitlesPreprocessing") \
    .getOrCreate()

# Load the Netflix titles dataset
netflix_df = spark.read.csv("netflix_titles.csv", header=True, inferSchema=True)

# Handling missing values
# Let's impute missing values in the "director" column with "Unknown"
netflix_mis = netflix_df.fillna({'director': 'Unknown'})
netflix_mis.show()
netflix_df = netflix_df.dropDuplicates()

netflix_df = netflix_df.withColumn("release_year", netflix_df["release_year"].cast("int"))
indexer = StringIndexer(inputCol="type", outputCol="type_index")
netflix_df = indexer.fit(netflix_df).transform(netflix_df)

# Next, let's encode the "country" column
indexer = StringIndexer(inputCol="country", outputCol="country_index")
netflix_df = indexer.fit(netflix_df).transform(netflix_df)
assembler = VectorAssembler(inputCols=["type_index", "country_index", "release_year"],
                            outputCol="features")
netflix_df = assembler.transform(netflix_df)

# Selecting relevant columns
netflix_df = netflix_df.select("features")

# Show the preprocessed DataFrame
netflix_df.show(truncate=False)

# Stop the SparkSession
spark.stop()

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.ml.feature import StringIndexer, VectorAssembler
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

# Create a SparkSession
spark = SparkSession.builder \
    .appName("NetflixTitlesModeling") \
    .getOrCreate()

# Load the Netflix titles dataset
netflix_df = spark.read.csv("netflix_titles.csv", header=True, inferSchema=True)

# Data Preprocessing
# Ensure that the "release_year" column is properly cast to integer
netflix_df = netflix_df.withColumn("release_year", netflix_df["release_year"].cast("integer"))

# Drop rows with missing values in the "release_year" column
netflix_df = netflix_df.na.drop(subset=["release_year"])

# Encode the "type" column into numeric format
indexer = StringIndexer(inputCol="type", outputCol="label")
netflix_df = indexer.fit(netflix_df).transform(netflix_df)

# Split the dataset into training and testing sets
train_df, test_df = netflix_df.randomSplit([0.8, 0.2], seed=42)

# Select features and target variable
assembler = VectorAssembler(inputCols=["release_year"], outputCol="features")
train_df = assembler.transform(train_df)
test_df = assembler.transform(test_df)

# Select an appropriate algorithm/model
# name of model (RandomForestClassifier)
rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=10)

# Train the model
model = rf.fit(train_df)

# Evaluate the model's performance
evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
train_accuracy = evaluator.evaluate(model.transform(train_df))
test_accuracy = evaluator.evaluate(model.transform(test_df))

print("Accuracy:", train_accuracy)

# Stop the SparkSession
spark.stop()